\chapter{The First 4 Moments}
\label{sec:moments}

\section{Moments and central moments}

For a random variable $X$, the expectation, also referred to as the first moment, is given by
\begin{align}
    \mu = \mathbb{E}(X)\notag
\end{align}
This expectation is estimated by the sample mean of the observed values $x$:
\begin{align}
    \hat{\mu} = \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i\notag
\end{align}
Variance serves as a measure of the dispersion of the random variable $X$. In the special case where $\mu = 0$, the variance simplifies to
\begin{align}
    \sigma^2 = \mathbb{E}(X^2)\notag
\end{align}
and is also referred to as the second moment. If $\mu \neq 0$, the variance is defined as
\begin{align}
    \label{eq:definition_variance}
    \sigma^2 &= \mathbb{E}((X-\mu)^2) \\
    &= \mathbb{E}(X^2) - \mathbb{E}(X)^2 \notag
\end{align}
which is also known as the centered second moment. The variance is estimated using
\begin{align}
    \hat{\sigma}^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2\notag
\end{align}
where the denominator $n-1$ represents Bessel's correction, which improves the estimation of variance (\cite{radziwillStatisticsEasierWay2017}). Analogously, the $r$-th moment is given by
\begin{align}
    \mathbb{E}(X^r)\notag
\end{align}
and the corresponding centered $r$-th moment is
\begin{align}
    \mathbb{E}((X-\mu)^r)\notag
\end{align}
Skewness measures the asymmetry of a distribution and is defined as the standardized third moment:
\begin{align}
    \label{eq:definition_skewness}
    \gamma_1 &= \frac{\mathbb{E}((X-\mu)^3)}{\sigma^3} \\
    &= \frac{\mathbb{E}(X^3) - 3\mathbb{E}(X)\mathbb{E}(X^2) + 2\mathbb{E}(X)^3}{\sigma^3} \notag
\end{align}
Different methods exist for estimating skewness, such as those proposed by Joanes \& Gill (\citeyear{joanesComparingMeasuresSample1998}):
\begin{align}
    b_1 &= \frac{\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^3}{\left[\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2\right]^{3/2}} \notag \\
    g_1 &= \frac{\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^3}{\left[\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2\right]^{3/2}} \notag \\
    G_1 &= \frac{n^2}{(n-1)(n-2)}b_1 = \frac{\sqrt{n(n-1)}}{n-2}g_1 \notag \\
    \hat{\gamma}_1 &= \frac{n}{(n-1)(n-2)}\sum_{i=1}^n \left(\frac{x_i-\bar{x}}{\hat{\sigma}}\right)^3 \notag
\end{align}
where $b_1$ and $g_1$ are estimators of the population skewness, while $G_1$ and $\hat{\gamma}_1$ estimate the skewness of a sample. The estimator $G_1$ is implemented in statistical software such as Excel, SAS, and SPSS (\cite{doaneMeasuringSkewnessForgotten2011}). Kurtosis measures the tailedness of a distribution and is defined as the standardized fourth moment:
\begin{align}
    \label{eq:definition_kurtosis}
    \gamma_2 &= \frac{\mathbb{E}((X-\mu)^4)}{\sigma^4} \\
    &= \frac{\mathbb{E}(X^4) - 4\mathbb{E}(X^3)\mathbb{E}(X) + 6\mathbb{E}(X^2)\mathbb{E}(X)^2 - 3\mathbb{E}(X)^4}{\sigma^4} \notag
\end{align}
Various estimation methods for kurtosis exist, such as those presented by Joanes \& Gill (\citeyear{joanesComparingMeasuresSample1998}):
\begin{align}
    g_2 &= \frac{\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^4}{\left[\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2\right]^2} \notag \\
    \hat{\gamma}_2 &= \frac{n(n+1)}{(n-1)(n-2)(n-3)}\sum_{i=1}^n \left(\frac{x_i-\bar{x}}{\hat{\sigma}}\right)^4 \notag
\end{align}
where $g_2$ estimates the kurtosis of a population and $G_2$ estimates the kurtosis of a sample. A commonly used alternative is excess kurtosis, which is obtained by subtracting 3:
\begin{align}
    \gamma_2^* &= \gamma_2 - 3\notag \\
    G_2 &= \frac{n-1}{(n-2)(n-3)}[(n+1)g_2 + 6] \notag
\end{align}
This adjustment is motivated by the fact that for a standard normally distributed variable $X$, the kurtosis is $\gamma_2 = 3$ and the excess kurtosis is $\gamma_2^* = 0$. In general, due to the high powers involved in the definitions of skewness and kurtosis, these estimators are highly sensitive to outliers. In the following, the term moments will be used broadly to include related measures such as variance, skewness, and kurtosis.

\section{Cumulants}

Throughout this work, the concept of cumulants will also be used, which provide an alternative representation of moments. The $r$-th cumulant is defined as the coefficient of $t^r$ in the logarithm of the moment generating function of $X$. The moment generating function of $X$ is given by
\begin{align}
    M_X(t) = \mathbb{E}(e^{tX})\notag
\end{align}
The cumulant generating function of $X$ is then defined as
\begin{align}
    K_X(t) = \log(M_X(t))\notag
\end{align} 
From this definition, the first four cumulants are given by
\begin{align}
    \label{eq:cumulants_1}
    \kappa_1 &= \mu\\
    \label{eq:cumulants_2}
    \kappa_2 &= \sigma^2\\
    \label{eq:cumulants_3}
    \kappa_3 &= \gamma_1\sigma^3\\
    \label{eq:cumulants_4}
    \kappa_4 &= \gamma_2^*\sigma^4
\end{align}

\section{Estimating the Moments of Low-Frequency Data using High-Frequency Data}

In contrast to the previously published methods Neuberger \& Payne (\citeyear{neubergerSkewnessStockMarket2021}) propose a novel method that only requires log prices to be martingales. Under these conditions, they define new moment measures that approximate the standard definitions:
\begin{align}
    var^L(r) &= \mathbb{E}(x^{(2L)}(r))\text{, where } x^{(2L)}(r) = 2(e^r-1-r) \notag \\
    var^E(r) &= \mathbb{E}(x^{(2E)}(r))\text{, where } x^{(2E)}(r) = 2(re^r-e^r+1) \notag \\
    skew(r) &= \frac{\mathbb{E}(x^{(3)}(r))}{var^L(r)^{3/2}}\text{, where } x^{(3)}(r) = 6((e^r+1)r - 2(e^r-1)) \notag \\
    kurt(r) &= \frac{\mathbb{E}(x^{(4)}(r))}{var^L(r)^2}\text{, where } x^{(4)}(r) = 12(r^2 + 2(e^r+2)r - 6(e^r-1)) \notag
\end{align}
where $r$ represents the log-return process:
\begin{align}
    r_t = \ln\left(\frac{P_t}{P_{t-1}}\right) \notag
\end{align}
The long-horizon returns process $R$ is defined as
\begin{align}
    R_t(T) &= \ln\left(\frac{P_t}{P_{t-T}}\right) \notag
\end{align}
This establishes a connection between low-frequency moments and high-frequency log returns:
\begin{align}
    var^L(R(T)) &= T\cdot var^L(r) \notag \\
    skew(R(T)) &= \left(skew(r) + 3\frac{cov(y^{(1)}, x^{(2E)}(r))}{var^L(r)^{3/2}}\right)T^{-1/2} \notag \\
    kurt(R(T)) &= \left(kurt(r) + 4\frac{cov(y^{(1)}, x^{(3)}(r))}{var^L(r)^2} + 6\frac{cov(y^{(2L)}, x^{(2L)}(r))}{var^L(r)^2}\right)T^{-1} \notag
\end{align}
where
\begin{align}
    y_t^{(j)} &= \sum_{u=1}^T \frac{x^{(j)}(R_{t-1}(u))}{T} \quad\text{for } j = 1,2L\notag \\
    \label{eq:x1}
    x^{(1)} &= e^r-1
\end{align}
where equation \eqref{eq:x1} originates from Neuberger (\citeyear{neubergerRealizedSkewness2012}). In Neuberger (\citeyear{neubergerRealizedSkewness2012}), the aggregation property is also introduced: if $g$ is a real function, $X$ is a process, and for times $0\le s\le t\le u\le T$,
\begin{align}
    \mathbb{E}_s(g(X_u-X_s)) = \mathbb{E}_s(g(X_u-X_t)) + \mathbb{E}_t(g(X_t-X_s))\notag
\end{align}
then the pair $(g,X)$ satisfies the aggregation property. This property is used, for instance, in estimating low-frequency variance by summing high-frequency variance. Fukasawa \& Matsushita (\citeyear{fukasawaRealizedCumulantsMartingales2021}) build on this idea and derive formulas for realized cumulants that also satisfy the aggregation property.