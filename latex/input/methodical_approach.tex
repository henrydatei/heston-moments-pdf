\section{Methodological Approach and Implementation}

\subsection{Simulation}
- all die in den vorherigen Kapiten vorgestellten Methoden wurden in Python implementiert
- Simulation von $n=1$ Paths des Heston-Models mit dem QE schema von Andersen (2008). Da hier über die Länge der Zeit eine Mittelung der Ergebnisse stattfindet, ist es nicht notwendig mehrere Pfade zu simulieren.
- Parameter der Simulation:
    - Interday 5 Minuten Preisdaten (79 pro Tag), 22 Handelstage pro Monat, 12 Monate pro Jahr, 15 Jahre ($T = 15$); insgesamt 312840 Preise pro Path
    - Burnin von 3 Jahren um Bias von Startpreis $S_0 = 100$ und Startvolatilität $v_0$ zu eleminieren
    - $v_0$: Start 0.01, Ende 0.5, Schrittweite 0.05
    - $\kappa$: Start 0.01, Ende 1, Schrittweite 0.1
    - $\theta$: Start 0.01, Ende 1, Schrittweite 0.05
    - $\sigma$: Start 0.01, Ende 1, Schrittweite 0.05
    - $\mu$: Nimmt nur die Werte 0 und 0.05 an
    - $\rho$: Start -0.9, Ende 0.9, Schrittweite 0.1
    - Insgesamt 1440000 Parameterkombinationen
- Es stellte sich in Vorversuchen heraus, dass das QE-Schema numerische Fehler produziert, wenn die Feller condition sehr deutlich nicht erfüllt war. Das zeichnete sich dadurch aus, dass sehr viele Preise den selben Wert haben, daher wurde beim ersten Pfad einer jeden Simulation gezählt, wie oft der häufigste Wert vorkommt. Wenn keine Fehler bei der Simulation auftreten liegt dieser Wert im unteren einstelligen Bereich, bei Fehlern in den Tausendern %TODO: prüfen
- Pro Pfad wurden die ersten 4 realisierten Momente, Skewness und Kurtosis nach Neuberger & Payne (2021) und die ersten 4 Kumulanten nach Fukasawa & Matsushita (2021) der 5-minütigen Returns ermittelt. Über die 100 Pfade werden diese Ergebnisse dann gemittelt, sodass man 4 Momente, 4 Kumulanten, Skewness und Kurtosis pro Simulation erhält. Dieser Prozess ist sehr zeitaufwendig, Rechenzeit beträgt etwa 100 Sekunden auf einem 1.8 GHz Dual-Core Intel Core i5. Es empfiehlt sich daher diese Simulationen zu parallelisieren und die Ergebnisse zu sammeln.
- Das Sammeln der Ergebnisse erfolgt in einer CSV-Datei und Log-Datei (insgesamt 50 Jobs à 256 Worker = 12800 Batches), um die parallele Schreibbelastung in einer Datei zu minimieren. Die einzelenen Daten werden dann zentral in einer Datenbank zusammengeführt.

\subsection{Calculating Results}
- für jede Simulation werden die Gram-Charlier Expansion, die Gram-Charlier Expansion mit positivity constraint, die Edgeworth Expansion und die Edgeworth Expansion mit positivity constraint, die Cornish-Fisher Expansion und die Saddlepoint Approximation berechnet
- jede Expansions-Methode wird auf die ersten 4 realisierten Momente und die ersten 4 Kumulanten angewendet
- für jede Simulation wird die theoretische Dichte aus der charakteristischen Funktion berechnet und mit der Dichte aus den Expansionen per Kolmogorov-Smirnov-Test und Cramer-von-Mises-Test verglichen
- diese Tests brauchen Samples, die aus der Verteilungsfunktion gezogen werden. Damit das immer funktioniert, wird aus der Dichte eine Verteilungsfunktion durch kumulative Summation erstellt und dann alle Werte durch die Summe geteilt. Damit ist das letzte Element der Verteilungsfunktion 1.
- Wie auch schon im Abschnitt zum Heston-Model angedeutet, führt die Formel \eqref{eq:heston_model_characteristic_function_C} zu overflow Fehlern, da Python nur mit Floats im Bereich von $10^{-308}$ bis $10^{308}$ rechnen kann. Daher wird der Term \eqref{eq:heston_model_characteristic_function_C_2} zur Berechnung verwendet.